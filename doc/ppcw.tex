\documentclass[twoside,11pt]{article}
\PassOptionsToPackage{hyphens}{url}
\usepackage{jmlr2e}
\usepackage{amsmath}
\usepackage[toc,page]{appendix}
\usepackage[table]{xcolor}
\usepackage[marginparsep=30pt]{geometry}
\usepackage{stmaryrd}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{tabu}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{listings}
\usepackage{fancyref}
\usepackage{relsize}
\usepackage{float}
\usepackage{subcaption}
\usepackage{diagbox}

\usetikzlibrary{%
    arrows,
    arrows.meta,
    decorations,
    backgrounds,
    positioning,
    fit,
    petri,
    shadows,
    datavisualization.formats.functions,
    calc,
    shapes,
    shapes.multipart,
    matrix,
    plotmarks
}

\usepgfplotslibrary{fillbetween, statistics}

\pgfplotsset{
  compat=1.3,
  every non boxed x axis/.style={
  enlarge x limits=false,
  x axis line style={}%-stealth},
  },
  every boxed x axis/.style={},
  every non boxed y axis/.style={
  enlarge y limits=false,
  y axis line style={}%-stealth},
  },
  every boxed y axis/.style={},
}

\def\perc{\texttt{perco\-late}}
\def\v{\texttt{v0.1.0}}

\def\titl{Performance Programming Coursework:
  Serial Optimization of a Molecual Dynamics Program}

\title{\titl}

\author{}

\ShortHeadings{B160509}{B160509}
\firstpageno{1}


\begin{document}

\maketitle

\begin{abstract}
\end{abstract}

\begin{keywords}
Scientific programming, serial optimization, molecular dynamics,
Fortran
\end{keywords}

\section{Introduction} % {{{
\label{sec:intro}

This paper documents the serial performance optimization conducted for
a molecular dynamics program written in the Fortran programming
language.
The program reads data from $n$ molecules as its input and iterates
a predefined amount of steps, which correspond to the progress of time
in the simulation.
Each step the data of the molecules (e.g.\ force, position in a three
dimensional space, velocity) is updated.
The program counts collisions between molecules and writes the
data for each molecule of the simulation out after a certain interval
of iterations.
This interval is called a superstep.
The original version of the program is not optimized for computational
efficiency.

This paper describes, documents and discusses the process of
optimizing the original version of the program serially.
The program was optimized for the Cirrus supercomputer, a tier-2
UK national supercomputer of the engineering and physical sciences
research council, which is hosted and maintained by the EPCC
\citep{cirrus}.
The compilers used were Intel's Fortran compiler \texttt{ifort},
version $18.0.5$ and $19.0.0$ for the linux operating system
\citep{ifort18, ifort19}.
The conducted optimizations range from choosing the appropriate
compiler flags over rewriting performance critical sections of the
program to hardware specific optimizations, like leveraging
vectorization and cache optimizations.
Since raw performance benefits are not all that is important for
writing well performing and good programs, the maintainability and
portability of the program are also looked at and discussed.

% TODO: hint on the results

This paper continues in Section~\ref{sec:md} with describing the
molecular dynamics program in detail.
Section~\ref{sec:setup} describes Cirrus and how the correctness of
the program is tested with a regression test suite.
Also the benchmark suite used for assessing the performance benefits
of an optimization is outlined.
Section~\ref{sec:opt} lists, describes and discusses all
optimizations tested with their performance benefits.
Afterwards, the results are discussed in Section~\ref{sec:dis}.
At least a conclusion is drawn in Section~\ref{sec:con}.

% }}}

\section{Molecular Dynamics Program} % {{{
\label{sec:md}

This section describes what the program does.
The program is a simple molecular dynamics program.
It reads data from $n$ particles from a file.
In the following, subscripts $1 \leq i \leq n$ refer to a particle
of the simulation.
The data for a particle $i$ is its mass $m_i$, the viscosity of
a fluid $i$ is part of $vis_i$, the position of the particle's center
in a three dimensional space $\vec{p}_i$ and its velocity $\vec{v}_i$.

The program iterates a predefined amount of iterations.
Each iteration represents a time step in the particle simulation.
The time is updated by a constant value $\Delta_t$.
Every step the position and the velocity of each particle is
updated, based on the gravitational forces operating on each particle.
The gravitational forces are the forces between each particle and the
gravitational force coming from a large central mass located at the
origin of the Euclidean space.
The particles are part of a fluid, which means the viscosity of
the fluid must also be taken into account.
The last external force operating on a particle is wind $\vec{w}$,
which is a constant vector over the whole simulation.

The gravitational forces are computed based on Newton's law of
universal gravitation.
It states, that any two physical objects attract each other
with a force, which is proportional to the mass of both objects and
inversely proportional to the squared distance between both
\citep{feynman1963}.
The scalar gravitational force $F$ between two objects 1 and 2 can be
mathematically described as:
\begin{align}
  F_{1,2} = G\frac{m_1 m_2}{||\vec{p}_1 - \vec{p}_2||_2^2},
\end{align}
where $G$ is the gravitational constant, $m_i$ the mass of object $i$
and $||\vec{p}_1 - \vec{p}_2||_2$ the Euclidean or $L_2$ distance
between the centers of both objects.
The vector form of the gravitational force object 2 operates on object
1, which also accounts for the direction of the force, is given by:
\begin{align}
  \label{eq:F}
  \vec{F}_{1\leftarrow2} = -F_{1,2}\frac{\vec{p}_1 - \vec{p}_2}
                   {||\vec{p}_1 - \vec{p}_2||_2}
          = -G\frac{m_1 m_2 (\vec{p}_1 - \vec{p}_2)}
                   {||\vec{p}_1 - \vec{p}_2||_2^3}.
\end{align}
The gravitational force, which object 1 operates on object 2 is the
additive inverse of $\vec{F}_{1 \leftarrow 2}$:
$\vec{F}_{2 \leftarrow 1} = -\vec{F}_{1 \leftarrow 2}$.
If particles collide, the gravitational forces between the two object
are negated.
The program finds collisions, by checking if the distance of two
particles is smaller than a threshold $\tau_{1,2}$, which is the sum
of the radii of the two particles.
For the program, all particles have a radius of $\frac{1}{2}$, so the
threshold for a collision is $1$.
Now we can define a pairwise gravitational forces function for the
particles of the simulation as:
\begin{align}
  \label{eq:f}
  f_{1 \leftarrow 2} := \begin{cases}
    \vec{F}_{1 \leftarrow 2} &\text{if }
      ||\vec{p}_1 - \vec{p}_2||_2 \ge \tau_{1,2} \\
    -\vec{F}_{1 \leftarrow 2} &\text{otherwise}
  \end{cases}.
\end{align}
Other than the pairwise gravitational forces between the particles,
there is the gravitational force of the central mass
$\vec{F}_{1 \leftarrow central}$.
The central mass lies at the origin of the three dimensional space,
which means its position vector $\vec{p}_{central} = 0$.

The viscosity of the fluid is another force operating on a particle.
It is simply the negative of the viscosity of the fluid multiplied
by the velocity vector of particle $i: -vis_i \vec{v}_i$.
Shear velocity of the fluid is not taken into account.
Lastly, there is the wind force, which is simply the negated viscosity
of the fluid particle $i$ is part of multiplied by the wind vector:
$-vis_i \vec{w}$.
The overall force per iteration operating on particle $i$ can now
be described as:
\begin{align}
  \vec{F}_i = -vis_i(\vec{v}_i + \vec{w}) +
              \vec{F}_{i \leftarrow central} +
              \sum_{j\neq i}^n f_{i \leftarrow j}.
\end{align}
Based on $\vec{F}_i$ we can now update $\vec{p}_i$ and $\vec{v}_i$:
\begin{align}
  \label{eq:p}
  \vec{p}_i &= \vec{p}_i + \Delta_t \vec{v}_i \\
  \label{eq:v}
  \vec{v}_i &= \vec{v}_i + \Delta_t \frac{\vec{F}_i}{m_i}.
\end{align}
The iterations of the program are broken down into supersteps.
On completion of a superstep, the updated particles are exported to a
file with the same format as the input file.

% }}}

\section{Setup} % {{{
\label{sec:setup}

This section outlines the settings, under which the program was
optimized for performance. Information about the used hardware is
given.
The way correctness of the program was tested is described.
Lastly the settings and the criterion for benchmarking the
computational performance are presented.

Like stated in Section~\ref{sec:intro}, the program was optimized for
the Cirrus supercomputer \citep{cirrus}.
Since we are running the program serially, we are not concerned with
the amount of nodes or the interconnect, but will focus on a single
compute node.
A single compute node of Cirrus contains two 2.1 GHz, 18-core Intel
Xeon E5-2695 processors (code name: Broadwell).
The processor supports the AVX2 vector instruction set \citep{avx2}.
Each processor is connected to 128 Gigabyte of memory.
Both processors are within a NUMA region, so 256 Gigabyte of memory
are actually at ones disposal \citep{cirrus_hardware}.
The compute node offers three levels of cache:
\begin{enumerate}
  \item 32 Kilobyte instruction and 32 Kilobyte of data cache
        (per core)
  \item 256 Kilobyte (per core)
  \item 45 Megabyte (shared)
\end{enumerate}

Testing the correctness of the program is not as straight-forward as
it seems at first glance.
Like stated in Section~\ref{sec:md}, after each superstep, the
updated particles are written out to file.
Comparing the output of the optimized version of the program with the
original one would be a sufficient test for correctness, if it were
not for floating point rounding errors.
These accumulate and after a certain amount of time, the numbers
generated by the optimized version will be too different from the
original ones.

In order to avoid getting different results, just because of floating
point rounding errors, the program was augmented by a special test
setting.
This test setting differs from the normal program, because it reads
the data it has written out after a superstep back in.
That way, the next superstep will work with the floating point numbers
that are crippled by writing them to file.
The floating point numbers are written to file text based in
exponential form with 16 digits, eight digits on the right side of the
decimal point \citep[see e.g.][for formatting IO in Fortran]
{fortran_formats}.
Changing the output format to a more precise representation is not
possible, because this would mean the files generated as output would
not have the same format as the input file with the initial states of
the particles.
The test setting allows for effectively comparing the output files of
both versions, because floating point rounding errors now only
accumulate over a single superstep instead of the whole simulation.

Once the discrepancy between the output values of the optimized
version and the original one surpasses a predefined error level,
the optimization is deemed to result in an incorrect version of the
program.
The predefined error level was set to be $0.05$.
If any output file contains a \texttt{NaN} value, the program is
also deemed incorrect.
The regression test suite was implemented with a Python script.

The program runs five supersteps.
Each superstep encompasses 100 iterations.
Computation is done using double precision floats.
The input file which was used for optimizing contained 4096 particles.
The goal of the optimization process was to reduce the wall-clock
time of the program to a minimum, while bearing in mind portability
and more importantly maintainability.
The program measures the time it needs for each superstep and its
overall time, including the file output.
In order to build the benchmark suite around the program, the timings
are exported to another file when the program has finished the
simulation.
The program was benchmarked by running it ten times on a compute node
of Cirrus and taking the average from those ten runs as the
performance measurement.
Running it ten times is sufficient to get a stable average,
because running the program as a job on a compute node of Cirrus
means exclusive hardware access to that node.
Only IO performance can be influenced by other users, because Cirrus
uses Lustre for its file system which is shared
\citep{cirrus_hardware}.
As will be shown below, IO performance is actually negligible when
it comes to performance optimization when compared to the
computational effort of the simulation.

% }}}

\section{Optimizations} % {{{
\label{sec:opt}

This section documents the process of performance optimization of the
program.
Focus lies more on the process, not the results.
All the successively performed optimizations are described.
Code quality in form of readability, portability and maintainability
is taken into account during the whole process and the optimizations
are all looked at from this perspective.
The optimization process can basically broken down into four phases:
(\romannumeral 1) rewriting the source code to Fortran 90,
restructuring the source code without changing the critical section,
(\romannumeral 2) enabling basic compiler optimizations,
(\romannumeral 3) rewriting the program for better performance and
(\romannumeral 4) trying out more advanced compiler optimizations on
the rewritten version of the program again.

\subsection{First Phase} % {{{

The first phase of optimization only concerns itself with increasing
the maintainability of the program.
The original version of the program is written in fixed format
Fortran \citep[see e.g.][for free vs.\ fixed format Fortran]
{fortran_free_fixed}.
Readability for screen based devices was deemed more of an issue than
formatting source code for punched cards, which are unfortunately not
supported by Cirrus.
So the first step was to reformat the source code to free format
Fortran to increase maintainability.

The original version of the program is spread across four files.
\texttt{control.f} contains the main program.
It performs initialization of the program.
This includes defining constants and reading the particles from the
initial file.
It contains the superstep loop and performs the output of the
intermediate states of the particles to file.
It also collects the timings for every superstep and the combined
time for all supersteps together.
The \texttt{MD.f} file contains the \texttt{evolve} subroutine.
This subroutine performs the main computations for the simulation.
It is called each superstep and iterates 100 times over the
simulation, updating the state of the particles.
The particle data is shared between the main program and the
\texttt{evolve} subroutine with a \texttt{COMMON} block
\citep[see e.g.][]{fortran_common}.
The \texttt{COMMON} block is defined in the \texttt{coord.inc} file,
which also contains the global constants $G$ and $m_{central}$.
Lastly, there is the \texttt{util.f} file containing utility
subroutines and functions, e.g.\ \texttt{visc\_force} or
\texttt{wind\_force}, which compute $-vis_i\vec{v}_i$ and
$-vis_i\vec{w}$ respectively (see Section~\ref{sec:md}).

Modern Fortran compilers like \texttt{ifort} version $18.0.5$ support
all Fortran 2008 features \citep{ifort18}.
Fortran introduced modules in Fortran 90, which make it much easier
to share data between subroutines and coupling can be much improved by
using them \citep{fortran_modules}.
Because using modules increases maintainability a lot, all routines
of the program are put into the main program in \texttt{control.f},
inside its \texttt{contains} block.
That way the particle data can be shared with the \texttt{evolve}
subroutine without a \texttt{COMMON} block.
These changes greatly increased maintainability, because of the
enhanced readability of the source code.
Also the build process is simplified, because only a single
Fortran 90 file needs to be compiled, rather than having to link
\texttt{control.f} and \texttt{MD.f} with \texttt{coord.inc}.

Both, the original version and the new Fortran 90 version were
modified to incorporate the test setting, where they read there
intermediate outputs back in in order to compute the next superstep.
In order for this setting not to interfere with the original setting
used for benchmarking, Intel's Fortran preprocessor was used
\citep{fpp}.
The additional reading back of the intermediate file is put into an
\texttt{\#ifdef} directive.
This way, the original version of the program used for benchmarking
is not damaged by additional checks at runtime.

Another aspect to consider in favor of the new version is the fact,
that the \texttt{COMMON} block is not aligned.
Figure~\ref{fig:common_unaligned} shows the compiler output, when
compiling the original version.
The \texttt{COMMON} block has alignment issues for the wind vector
$\vec{w}$, which could have an impact on the programs performance.
Removing the block removes the alignment issue.
So not only is the new version better maintainable, it also removes
the first performance issue with the original version.

Both versions were compiled using \texttt{ifort} version $18.0.5$ with
the following compiler flags which influence performance:
\texttt{-O0}, which disables any compiler optimization,
\texttt{-no-vec}, which inhibits vectorization and
\texttt{-check uninit,bounds}, which tells the compiler to add
extra instructions to the program which perform explicit checking
for uninitialized variables and out-of-bounds access of arrays.

Benchmarking the original version reveals that it takes on average
$1270$ seconds for all five supersteps to complete.
A single superstep takes on average $254$ seconds to complete.
If one subtracts the sum of the individual timings of all five
supersteps from the overall time, one gets the time spent doing the
file output.
The IO time lies at a quarter of a second for the original version,
which is $0.02\%$ of the overall runtime.
The new Fortran 90 version of the program takes only $1110$ seconds
on average to complete.
The average superstep time lies at $222$ seconds.
While the focus of the first phase of the optimization actually was
about enhancing maintainability and setting the right foundation for
the next phases, the performance was already increased by $12.5\%$.

\begin{figure}
\begin{verbatim}
ifort -g -O0  -check uninit,bounds -no-vec -fpp
-o ../bin/old_bench control.f MD.o util.o

./coord.inc(25): remark #6375: Because of COMMON, the alignment of
object is inconsistent with its type - potential performance
impact. [WIND]

      DOUBLE PRECISION wind(Ndim)
-----------------------^
\end{verbatim}
\caption{Output from \texttt{ifort} when compiling the original
  version of the program. The constant vector $\vec{w}$ is not
  aligned.}
\label{fig:common_unaligned}
\end{figure}

% }}}

\subsection{Second Phase} % {{{

The second phase was about using the more common compiler flags to
enhance the performance of the Fortran 90 version.
The compiler flags used in phase one not only hinder compiler
optimizations with \texttt{-O0}, they even make the code perform worse
by adding the out-of-bounds access and uninitialized variables flags.
Therefore the first step to better performing code was to utilize the
compiler.
This phase was not about finding a definitive set of flags, but only
a first step to see how much the compiler can achieve using the
more common compiler flags for optimization.
It was more motivated by the still horrible runtime of 1110 seconds,
which hinders rapid development during the third phase.
Table~\ref{tab:p2} shows all the different flags tried and how they
impacted performance.

The first act was to remove the unnecessary checks for out-of-bounds
access of arrays and using uninitialized variables.
The Fortran 90 version of the code is riddled with loops, so removing
the checks for each should have quite the impact on performance.
As it turned out it did.
Removing the checks improved the average overall time by $33\%$.
That means $1/3$ of the time was spent checking for out-of-bounds
access and the use of uninitialized variables.

The next step was to gradually increase the level of compiler
optimization from \texttt{-O0} to \texttt{-O3}.
The first level of optimization \texttt{-O1} enables speed
optimizations that do not enlarge binary size.
Optimizations done include data-flow analysis, test replacement and
instruction scheduling.
\texttt{-O1} is designed for large codes with many branches that
are not loops \citep{o}.
While this description does not fit to the program at all, which
has only one significant branch (code structure is discussed below)
and spends most of its time in loops, \texttt{-O1} still increases
the performance by 44\%.
Average overall time is reduced from 734 seconds to 413 seconds
(see Table~\ref{tab:p2}).

\texttt{-O2} is the recommended level of compiler optimization.
It performs basic loop optimizations like interchanging, unrolling or
scalar replacements.
Furthermore inlining, intra-file ipo (interprocedural optimization),
dead code elimination and many more optimizations are enabled
\citep[see][]{o}.
Enabling \texttt{-O2} reduced the average overall time down to
85 seconds, which is 79\% better than the program compiled with
\texttt{-O1} (see Table~\ref{tab:p2}).

\texttt{-O3} enables more aggressive optimizations concerning loops
and memory access transformations, additionally to the optimizations
done using \texttt{-O2}.
Optimizations include loop fusion and collapsing if statements.
It is the recommended level of optimization for floating point
operation heavy programs that spend a lot of time in loops
\citep{o}.
This exactly describes the molecular dynamics program and
\texttt{-O3} actually increases the performance further.
The average overall time was further reduced from 85 seconds down
to 62 seconds.
This is an additional 27\% improvement over \texttt{-O2}
(see Table~\ref{tab:p2}).

At this point, vectorization was still disabled with the
\texttt{-no-vec} flag.
Enabling vectorization on the yet unoptimized Fortran 90 program
resulted in a regression of the average overall time of 33\%
(see Table~\ref{tab:p2}).
Like described above, the second phase is only about finding a
set of compiler optimizations that would enable a more rapid analysis
of the performance in the crucial third phase.
This is the reason why the drop in performance was not analyzed
further.
For the unoptimized Fortran 90 version of the program somehow
vectorization seems to cancel optimizations from \texttt{-O3}.
The guess at this point was that the program profits more from
optimizations from \texttt{-O3} (like loop unrolling) than
from being vectorized.

Lastly other common flags for compiler optimization were considered.
Two recommended options besides \texttt{-O3} are \texttt{-ipo} and
\texttt{-xHOST} \citep{user389}.
Neither would improve the performance of the program.
\texttt{-ipo} enables interprocedural optimizations between files
\citep{ipo}.
The program only consists of a single file, so \texttt{-ipo} would
not improve performance.
\texttt{-xHOST} forces the compiler to generate instructions from the
highest instruction set supproted by the host \citep{xhost}.
The host is a frontend node of Cirrus in this case.
The frontend nodes of Cirrus are the same as its compute nodes.
The highest instruction set supported is AVX2
(see Section~\ref{sec:setup}).
\texttt{-no-vec}, which is still enabled at this point, cancels out
\texttt{-xHOST}.

Lastly \texttt{-Ofast} was tested.
\texttt{-Ofast} is a shorthand compiler flag, which combines
\texttt{-O3} with a faster floating point model than the default one.
It sets \texttt{-O3}, \texttt{-no-prec-div} and
\texttt{-fp-model fast=2} \citep{ofast}.
\texttt{-no-prec-div} increases the speed of floating point divisions.
The cost of this flag is a reduction in precision \citep{no_prec_div}.
\texttt{-fp-model fast=2} works the same way.
It increases the performance on the cost of less precise results of
floating point operations \citep{fp_model}.
Enabling \texttt{-Ofast} does not result in less accurate test
results.
The program's correctness is still given, even though floating point
precision was lowered.
\texttt{-Ofast} does not increase the performance compared to
\texttt{-O3} (see Table~\ref{tab:p2}).

Phase two was terminated at this point.
The best compiler flags determined were \texttt{-O3} with
\texttt{-no-vec}.
The average overall time after phase two is 62 seconds.
This is an improvement of a staggering 94\% over the results after
phase one (1110 seconds), simply by enabling compiler optimization
and removing unnecessary checks.

\begin{table}
  \begin{tabu}{|l|X|l|X|X|}
    \hline
    Optimization &$\emptyset$ overall time
                 &$\emptyset$ superstep time  &$+/-\%$ &Status \\
    \hline
    Removed checks &734s &147s &33\% &Improvement \\
    \hline
    \texttt{-O1} &413s &83s &44\% &Improvement \\
    \hline
    \texttt{-O2} &85s &17s &79\% &Improvement \\
    \hline
    \texttt{-O3} &62s &12s &27\% &Improvement \\
    \hline
    Removed \texttt{-no-vec} &82s &16s &-33\% &Regression \\
    \hline
    \texttt{-Ofast} &62s &12s &0\% &Invariant \\
    \hline
  \end{tabu}
  \caption{Compiler flags tried during the second phase of
    optimization. The $+/-\%$ column displays the
    variation in average overall time from the best version of the
    program so far. For example, the best version for the removal of
    the extra checks was the Fortran 90 version from phase one.
    For \texttt{-Ofast} the best version of the code was the one
    compiled without the checks and with \texttt{-O3}.
  }
  \label{tab:p2}
\end{table}

% }}}

\subsection{Third Phase} % {{{

The third phase was the most crucial phase.
While the second phase already improved the performance by 94\%,
the quality of the program is still bad.
Both in consideration of performance and more importantly
maintainability.
Phase three of the optimization efforts therefore tackles this
problem by rewriting the critical section of the program, which spans
the computations for updating the particles at each time step.

The third phase can be loosely split into two sub-phases.
The first sub-phase is only about making code more readable, e.g.\
writing idiomatic Fortran code instead of loops, removing unnecessary
variables and computation and fusing loops by hand.
This step was necessary not only for increasing maintainability, but
also to make the second sub-phase easier to implement.
The second sub-phase was about hardware and environment specific
optimizations, mainly enabling vectorization.

The common workflow for optimizing for speed normally consists of
iterations, where the program is profiled, the bottleneck determined
and then optimized.
The workflow used here is less vigorous.
Profiling for identification of bottlenecks is not necessary,
because it is well established where the bottleneck of molecular
dynamics programs is: computing the pairwise forces between the
particles \citep{chiu_et_al_2011}.
Also, like described above, the guiding principle for rewriting the
program was increased maintainability rather than pure speed.
The guiding tool for the environment specific optimizations was the
optimization report generated by the compiler.

\begin{algorithm} % {{{
  \caption{: original computation per time step}
  \label{alg:old}

  \begin{algorithmic}[1]
    \FOR{i=1,\dots,n}
      \STATE{$\vec{F}_i$ := $-vis_i \vec{v}_i$}
      \COMMENT{Compute viscosity force for particle $i$}
    \ENDFOR

    \FOR{i=1,\dots,n}
      \STATE{$\vec{F}_i$ := $\vec{F}_i - vis_i \vec{w}$}
      \COMMENT{Compute wind force for particle $i$}
    \ENDFOR

    \FOR{i=1,\dots,n}
      \STATE{$\vec{r}_i$ := $||\vec{p}_i||_2$}
      \COMMENT{$\vec{r}$ is used in loop below for the denominator in
        $\vec{F}_{i \leftarrow central}$}
    \ENDFOR

    \FOR{i=1,\dots,n}
      \STATE{$\vec{F}_i$ := $\vec{F}_i +
        \vec{F}_{i \leftarrow central}$}
      \COMMENT{see Equation~\ref{eq:F}}
    \ENDFOR
    \STATE{Compute pairwise forces with
      Algorithm~\ref{alg:pairwise_old}}

    \FOR{i=1,\dots,n}
      \STATE{$\vec{p}_i$ := $\vec{p}_i + \Delta_t \vec{v}_i$}
      \COMMENT{Update position vector of particle $i$
        (see Equation~\ref{eq:p})}
    \ENDFOR

    \FOR{i=1,\dots,n}
      \STATE{$\vec{v}_i$ := $\vec{v}_i + \Delta_t
        \vec{F}_i / m_i$}
      \COMMENT{Update velocity vector of particle $i$
        (see Equation~\ref{eq:v})}
    \ENDFOR

  \end{algorithmic}
\end{algorithm} % }}}

\begin{algorithm} % {{{
  \caption{: original pairwise forces computation}
  \label{alg:pairwise_old}

  \begin{algorithmic}[1]

    \FOR{i=1,\dots,n}
      \FOR{j=i+1,\dots,n}
        \STATE{$\Delta\vec{p}_{i,j}$ := $\vec{p}_i - \vec{p}_j$}
      \ENDFOR
    \ENDFOR

    \FOR{i=1,\dots,n}
      \FOR{j=i+1,\dots,n}
        \STATE{$\Delta\vec{p}_{i,j}$ := $||\Delta\vec{p}_{i,j}||_2$}
        \COMMENT{$\Delta\vec{p}_{i,j}$ is used in loop below for the
          denominator in $f_{i \leftarrow j}$}
      \ENDFOR
    \ENDFOR

    \FOR{i=1,\dots,n}
      \FOR{j=i+1,\dots,n}
        \STATE{$\vec{F}_i := \vec{F}_i + f_{i \leftarrow j}$}
        \COMMENT{see Equation~\ref{eq:f}}
        \STATE{$\vec{F}_j := \vec{F}_j - f_{i \leftarrow j}$}
        \COMMENT{see Equation~\ref{eq:f}}
      \ENDFOR
    \ENDFOR

  \end{algorithmic}
\end{algorithm} % }}}

% }}}

\subsection{Fourth Phase} % {{{

% }}}


% journey through time

% discuss every thought I had

% use pseudo code and small fortran snippets for visualization

% make it clear that I could use common sense most of the time,
% but include vtune results and io time

% }}}

\section{Discussion} % {{{
\label{sec:dis}

% }}}

\section{Conclusion} % {{{
\label{sec:con}

% }}}

\bibliography{library.bib}

\end{document}
